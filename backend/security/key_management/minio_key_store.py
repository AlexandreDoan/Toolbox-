"""
Stockage s√©curis√© des cl√©s dans MinIO - T√¢che 23
Compatible avec l'architecture PostgreSQL existante
"""
import os
import json
from datetime import datetime
from typing import Optional, Dict, List
from minio import Minio
from minio.error import S3Error
from io import BytesIO
import logging

logger = logging.getLogger('toolbox.keys')

class MinIOKeyStore:
    """Stockage s√©curis√© des cl√©s dans MinIO (T√¢che 23)"""
    
    def __init__(self, minio_client: Minio):
        self.client = minio_client
        self.bucket = 'encryption-keys'
        self._ensure_bucket_security()
    
    def _ensure_bucket_security(self):
        """Configure les politiques de s√©curit√© du bucket keys"""
        try:
            # V√©rifier que le bucket existe
            if not self.client.bucket_exists(self.bucket):
                self.client.make_bucket(self.bucket)
                logger.info(f"üîí Bucket s√©curis√© cr√©√©: {self.bucket}")
            
            # Note: Les politiques MinIO avanc√©es n√©cessitent un setup plus complexe
            # Pour un projet ESI M1, on se concentre sur l'impl√©mentation fonctionnelle
            logger.info(f"üîí Bucket de cl√©s s√©curis√©: {self.bucket}")
            
        except S3Error as e:
            logger.warning(f"‚ö†Ô∏è Impossible de configurer la s√©curit√© du bucket: {e}")
    
    def store_key(self, key_id: str, key_data: str, metadata: Dict = None) -> bool:
        """Stocke une cl√© dans MinIO avec m√©tadonn√©es"""
        try:
            # M√©tadonn√©es par d√©faut conformes au cahier des charges
            meta = {
                'created_at': datetime.now().isoformat(),
                'key_type': 'fernet',
                'algorithm': 'AES-128',
                'status': 'active',
                'created_by': 'toolbox_system'
            }
            
            if metadata:
                meta.update(metadata)
            
            # Stockage s√©curis√© dans MinIO
            key_path = f'keys/{key_id}.key'
            key_bytes = key_data.encode()
            self.client.put_object(
                self.bucket,
                key_path,
                BytesIO(key_bytes),
                length=len(key_bytes),
                metadata=meta
            )

            # Log de l'op√©ration pour audit
            self._log_key_operation(key_id, 'store', success=True)
            logger.info(f"üîë Cl√© stock√©e dans MinIO: {key_id}")
            return True
            
        except S3Error as e:
            logger.error(f"‚ùå Erreur stockage cl√© {key_id}: {e}")
            self._log_key_operation(key_id, 'store', success=False, error=str(e))
            return False
    
    def retrieve_key(self, key_id: str) -> Optional[str]:
        """R√©cup√®re une cl√© depuis MinIO"""
        try:
            key_path = f'keys/{key_id}.key'
            response = self.client.get_object(self.bucket, key_path)
            key_data = response.read().decode()
            
            # Log de l'acc√®s pour audit (conforme t√¢che 23)
            self._log_key_operation(key_id, 'retrieve', success=True)
            logger.debug(f"üîë Cl√© r√©cup√©r√©e: {key_id}")
            
            return key_data
            
        except S3Error as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration cl√© {key_id}: {e}")
            self._log_key_operation(key_id, 'retrieve', success=False, error=str(e))
            return None
    
    def list_keys(self) -> List[Dict]:
        """Liste toutes les cl√©s avec m√©tadonn√©es"""
        try:
            keys = []
            objects = self.client.list_objects(self.bucket, prefix='keys/', recursive=True)
            
            for obj in objects:
                if obj.object_name.endswith('.key'):
                    # R√©cup√©rer les m√©tadonn√©es
                    try:
                        stat = self.client.stat_object(self.bucket, obj.object_name)
                        
                        key_info = {
                            'key_id': obj.object_name.replace('keys/', '').replace('.key', ''),
                            'size': obj.size,
                            'last_modified': obj.last_modified.isoformat() if obj.last_modified else None,
                            'metadata': stat.metadata or {}
                        }
                        keys.append(key_info)
                    except S3Error as e:
                        logger.warning(f"‚ö†Ô∏è Impossible de r√©cup√©rer m√©tadonn√©es: {obj.object_name}: {e}")
            
            logger.debug(f"üìã {len(keys)} cl√©s list√©es")
            return keys
            
        except S3Error as e:
            logger.error(f"‚ùå Erreur listage cl√©s: {e}")
            return []
    
    def archive_key(self, key_id: str) -> bool:
        """Archive une cl√© (d√©place vers archive/) - Rotation s√©curis√©e"""
        try:
            source_path = f'keys/{key_id}.key'
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            archive_path = f'archive/{key_id}_{timestamp}.key'
            
            # Copier vers archive
            self.client.copy_object(
                self.bucket, 
                archive_path,
                f'/{self.bucket}/{source_path}'
            )
            
            # Supprimer l'original
            self.client.remove_object(self.bucket, source_path)
            
            # Log de l'archivage
            self._log_key_operation(key_id, 'archive', success=True, 
                                  details={'archive_path': archive_path})
            
            logger.info(f"üì¶ Cl√© archiv√©e: {key_id} ‚Üí {archive_path}")
            return True
            
        except S3Error as e:
            logger.error(f"‚ùå Erreur archivage cl√© {key_id}: {e}")
            self._log_key_operation(key_id, 'archive', success=False, error=str(e))
            return False
    
    def delete_key(self, key_id: str) -> bool:
        """Supprime d√©finitivement une cl√© (admin seulement)"""
        try:
            key_path = f'keys/{key_id}.key'
            self.client.remove_object(self.bucket, key_path)
            
            # Log de la suppression
            self._log_key_operation(key_id, 'delete', success=True)
            logger.warning(f"üóëÔ∏è Cl√© supprim√©e d√©finitivement: {key_id}")
            return True
            
        except S3Error as e:
            logger.error(f"‚ùå Erreur suppression cl√© {key_id}: {e}")
            self._log_key_operation(key_id, 'delete', success=False, error=str(e))
            return False
    
    def _log_key_operation(self, key_id: str, operation: str, success: bool, 
                          error: str = None, details: Dict = None):
        """Log des op√©rations sur les cl√©s pour audit (conforme t√¢che 23)"""
        audit_log = {
            'timestamp': datetime.now().isoformat(),
            'key_id': key_id,
            'operation': operation,
            'success': success,
            'user': 'system',  # √Ä am√©liorer avec le syst√®me d'auth existant
            'details': details or {}
        }
        
        if error:
            audit_log['error'] = error
        
        try:
            # Stocker le log d'audit dans MinIO
            date_str = datetime.now().strftime('%Y%m%d')
            log_name = f'audit/{date_str}/{key_id}_{operation}_{int(datetime.now().timestamp())}.json'
            
            log_data = json.dumps(audit_log, indent=2).encode()
            self.client.put_object(
                self.bucket,
                log_name,
                BytesIO(log_data),
                length=len(log_data)
            )

        except S3Error:
            # En cas d'erreur, au moins logger localement
            logger.warning(f"‚ö†Ô∏è Impossible de stocker l'audit pour {key_id}: {operation}")
    
    def get_audit_logs(self, key_id: str = None, date: str = None) -> List[Dict]:
        """R√©cup√®re les logs d'audit"""
        try:
            logs = []
            prefix = 'audit/'
            
            if date:
                prefix += f'{date}/'
            
            objects = self.client.list_objects(self.bucket, prefix=prefix, recursive=True)
            
            for obj in objects:
                if obj.object_name.endswith('.json'):
                    if not key_id or key_id in obj.object_name:
                        try:
                            response = self.client.get_object(self.bucket, obj.object_name)
                            log_data = json.loads(response.read().decode())
                            logs.append(log_data)
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Erreur lecture log {obj.object_name}: {e}")
            
            return sorted(logs, key=lambda x: x['timestamp'], reverse=True)
            
        except S3Error as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration logs audit: {e}")
            return []
